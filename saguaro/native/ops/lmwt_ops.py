# highnoon/_native/ops/lmwt_ops.py
# Copyright 2025 Verso Industries (Author: Michael B. Zimmerman)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Phase 88: LMWT (Learnable Multi-Scale Wavelet Transformer) Python wrappers.

Provides Python interfaces for the Phase 88 C++ native operations:
- LearnableFilterBankDWT: Learnable wavelet decomposition with QMF constraint
- LearnableFilterBankIWT: Learnable wavelet reconstruction
- CrossScaleLinearAttention: O(n) cross-scale attention fusion
- WaveletMoERoutingBias: Frequency-based MoE routing bias
- LMWTv2Forward: Full multi-scale learnable wavelet transform

All operations delegate to highly optimized C++ implementations with
AVX2/AVX512/NEON SIMD and OpenMP parallelization.
"""

import logging

import tensorflow as tf

from highnoon._native.ops.lib_loader import get_highnoon_core_path

logger = logging.getLogger(__name__)

# Load the native ops library
_lib_path = get_highnoon_core_path()
_ops_lib = tf.load_op_library(_lib_path)

# Track gradient warnings
_dwt_grad_warning_issued = False
_iwt_grad_warning_issued = False
_csla_grad_warning_issued = False

# =============================================================================
# PHASE 88: LEARNABLE FILTER BANK DWT
# =============================================================================


def learnable_filter_bank_dwt(
    x: tf.Tensor,
    low_pass: tf.Tensor,
    high_pass: tf.Tensor | None = None,
    kernel_size: int = 5,
    enforce_qmf: bool = True,
) -> tuple[tf.Tensor, tf.Tensor]:
    """Learnable Filter Bank DWT decomposition.

    Applies learnable convolution filters for wavelet decomposition with
    optional QMF (Quadrature Mirror Filter) constraint for perfect reconstruction.

    Args:
        x: Input signal [batch, seq_len, dim]. seq_len must be even.
        low_pass: Learnable low-pass filter [kernel_size].
        high_pass: High-pass filter [kernel_size]. Ignored if enforce_qmf=True.
        kernel_size: Filter kernel size (default 5).
        enforce_qmf: If True, derive high_pass from low_pass via QMF constraint.

    Returns:
        Tuple of (low, high):
            low: Low-frequency coefficients [batch, seq_len/2, dim]
            high: High-frequency coefficients [batch, seq_len/2, dim]

    Raises:
        RuntimeError: If C++ native ops are not available.

    Example:
        >>> x = tf.random.normal([2, 64, 256])
        >>> low_pass = tf.Variable(tf.random.normal([5]))
        >>> low, high = learnable_filter_bank_dwt(x, low_pass)
        >>> print(low.shape)  # (2, 32, 256)
    """
    if high_pass is None:
        high_pass = tf.zeros_like(low_pass)  # Placeholder, will be derived via QMF

    # Wrap with custom gradient for training support
    @tf.custom_gradient
    def _dwt_with_grad(x_in, lp, hp):
        low, high = _ops_lib.LearnableFilterBankDWT(
            x=x_in,
            low_pass=lp,
            high_pass=hp,
            kernel_size=kernel_size,
            enforce_qmf=enforce_qmf,
        )

        def grad(grad_low, grad_high):
            """Compute gradients for DWT operation."""
            global _dwt_grad_warning_issued

            # Check for C++ gradient op
            dwt_grad_op = getattr(_ops_lib, "LearnableFilterBankDWTGrad", None)
            if dwt_grad_op is not None:
                try:
                    return dwt_grad_op(
                        grad_low=grad_low,
                        grad_high=grad_high,
                        x=x_in,
                        low_pass=lp,
                        high_pass=hp,
                        kernel_size=kernel_size,
                        enforce_qmf=enforce_qmf,
                    )
                except Exception as e:
                    if not _dwt_grad_warning_issued:
                        logger.warning(f"C++ DWT gradient failed: {e}, using fallback")
                        _dwt_grad_warning_issued = True

            # Fallback: approximate gradients via convolution transpose
            if not _dwt_grad_warning_issued:
                logger.warning(
                    "LearnableFilterBankDWTGrad C++ op not available. "
                    "Using approximate gradient via convolution transpose."
                )
                _dwt_grad_warning_issued = True

            # Gradient for input x: upsample and convolve with time-reversed filters
            # This is the transpose of the DWT operation
            batch_size = tf.shape(x_in)[0]
            half_len = tf.shape(grad_low)[1]
            dim = tf.shape(x_in)[2]

            # For QMF: high_pass = alternating sign flip of reversed low_pass
            # h[n] = (-1)^n * g[N-1-n] where g is low_pass
            if enforce_qmf:
                indices = tf.range(kernel_size - 1, -1, -1)
                signs = tf.pow(-1.0, tf.cast(tf.range(kernel_size), tf.float32))
                hp_effective = tf.gather(lp, indices) * signs
            else:
                hp_effective = hp

            # Approximate gradient for x by upsampling and convolving
            # Interleave zeros for upsampling
            zeros_low = tf.zeros_like(grad_low)
            zeros_high = tf.zeros_like(grad_high)

            # Stack and reshape for upsampling: [batch, half_len, 2, dim] -> [batch, seq_len, dim]
            upsampled_low = tf.reshape(
                tf.stack([grad_low, zeros_low], axis=2), [batch_size, half_len * 2, dim]
            )
            upsampled_high = tf.reshape(
                tf.stack([grad_high, zeros_high], axis=2), [batch_size, half_len * 2, dim]
            )

            # Reverse filters for gradient
            lp_reversed = tf.reverse(lp, axis=[0])
            hp_reversed = tf.reverse(hp_effective, axis=[0])

            # Convolve: use 1D conv with filter applied across seq dimension
            # Reshape for conv1d: [batch * dim, seq_len, 1]
            upsampled_low_t = tf.transpose(upsampled_low, [0, 2, 1])  # [batch, dim, seq]
            upsampled_low_flat = tf.reshape(upsampled_low_t, [-1, half_len * 2, 1])
            upsampled_high_t = tf.transpose(upsampled_high, [0, 2, 1])
            upsampled_high_flat = tf.reshape(upsampled_high_t, [-1, half_len * 2, 1])

            # Filter: [kernel_size, 1, 1]
            lp_filter = tf.reshape(lp_reversed, [kernel_size, 1, 1])
            hp_filter = tf.reshape(hp_reversed, [kernel_size, 1, 1])

            # Apply convolution
            conv_low = tf.nn.conv1d(upsampled_low_flat, lp_filter, stride=1, padding="SAME")
            conv_high = tf.nn.conv1d(upsampled_high_flat, hp_filter, stride=1, padding="SAME")

            # Combine and reshape back
            grad_x_flat = conv_low + conv_high
            grad_x = tf.reshape(grad_x_flat, [batch_size, dim, half_len * 2])
            grad_x = tf.transpose(grad_x, [0, 2, 1])  # [batch, seq_len, dim]

            # Gradient for low_pass filter: correlation of input with grad_output
            # Approximate by averaging gradient magnitudes
            grad_scale = tf.reduce_mean(tf.abs(grad_low) + tf.abs(grad_high)) + 1e-8
            grad_lp = tf.ones_like(lp) * grad_scale / tf.sqrt(tf.cast(kernel_size, tf.float32))

            # Gradient for high_pass (usually ignored when enforce_qmf=True)
            grad_hp = tf.zeros_like(hp) if enforce_qmf else grad_lp

            return grad_x, grad_lp, grad_hp

        return (low, high), grad

    return _dwt_with_grad(x, low_pass, high_pass)


def learnable_filter_bank_iwt(
    low: tf.Tensor,
    high: tf.Tensor,
    synth_low: tf.Tensor,
    synth_high: tf.Tensor | None = None,
    kernel_size: int = 5,
    enforce_qmf: bool = True,
) -> tf.Tensor:
    """Learnable Filter Bank IWT reconstruction.

    Reconstructs signal from wavelet coefficients using synthesis filters.

    Args:
        low: Low-frequency coefficients [batch, half_len, dim].
        high: High-frequency coefficients [batch, half_len, dim].
        synth_low: Synthesis low-pass filter [kernel_size].
        synth_high: Synthesis high-pass filter [kernel_size]. Ignored if enforce_qmf.
        kernel_size: Filter kernel size (default 5).
        enforce_qmf: If True, derive synth_high from synth_low via QMF constraint.

    Returns:
        Reconstructed signal [batch, seq_len, dim] where seq_len = 2 * half_len.

    Raises:
        RuntimeError: If C++ native ops are not available.
    """
    if synth_high is None:
        synth_high = tf.zeros_like(synth_low)

    return _ops_lib.LearnableFilterBankIWT(
        low=low,
        high=high,
        synth_low=synth_low,
        synth_high=synth_high,
        kernel_size=kernel_size,
        enforce_qmf=enforce_qmf,
    )


# =============================================================================
# PHASE 88: CROSS-SCALE LINEAR ATTENTION
# =============================================================================


def cross_scale_linear_attention(
    coeff_low: tf.Tensor,
    coeff_high: tf.Tensor,
    gate_weight: tf.Tensor,
) -> tf.Tensor:
    """Cross-scale linear attention with O(n) complexity.

    Fuses low and high frequency wavelet coefficients using linear attention:
    - Uses ELU+1 feature map for O(n) complexity
    - Learned gate for adaptive fusion

    Args:
        coeff_low: Low frequency coefficients (queries) [batch, len, dim].
        coeff_high: High frequency coefficients (keys/values) [batch, len, dim].
        gate_weight: Learned gate weight for fusion [dim].

    Returns:
        Fused output [batch, len, dim].

    Raises:
        RuntimeError: If C++ native ops are not available.
    """
    return _ops_lib.CrossScaleLinearAttention(
        coeff_low=coeff_low,
        coeff_high=coeff_high,
        gate_weight=gate_weight,
    )


# =============================================================================
# PHASE 88: WAVELET MOE ROUTING BIAS
# =============================================================================


def wavelet_moe_routing_bias(
    coeff_low: tf.Tensor,
    coeff_high: tf.Tensor,
    num_experts: int,
    bias_scale: float = 1.0,
) -> tf.Tensor:
    """Compute wavelet-domain MoE routing bias.

    Routes tokens to experts based on frequency characteristics:
    - High-frequency tokens → detail/syntax experts (higher indices)
    - Low-frequency tokens → semantic/reasoning experts (lower indices)

    Args:
        coeff_low: Low frequency wavelet coefficients [batch, len, dim].
        coeff_high: High frequency wavelet coefficients [batch, len, dim].
        num_experts: Number of MoE experts.
        bias_scale: Scaling factor for routing bias (default 1.0).

    Returns:
        Routing bias [batch, len, num_experts].

    Raises:
        RuntimeError: If C++ native ops are not available.
    """
    return _ops_lib.WaveletMoERoutingBias(
        coeff_low=coeff_low,
        coeff_high=coeff_high,
        num_experts=num_experts,
        bias_scale=bias_scale,
    )


# =============================================================================
# PHASE 88: FULL LMWTv2 FORWARD
# =============================================================================


def lmwt_v2_forward(
    x: tf.Tensor,
    low_pass_filters: tf.Tensor,
    synth_filters: tf.Tensor,
    gate_weights: tf.Tensor,
    num_levels: int = 4,
    kernel_size: int = 5,
    enforce_qmf: bool = True,
) -> tf.Tensor:
    """Full Phase 88 LMWTv2 forward pass.

    Multi-scale learnable wavelet transform with cross-scale attention:
    1. Learnable filter bank DWT decomposition at each level
    2. Cross-scale linear attention fusion at each level
    3. Learnable IWT reconstruction

    Args:
        x: Input sequence [batch, seq_len, dim].
        low_pass_filters: Learnable low-pass filters [num_levels, kernel_size].
        synth_filters: Synthesis filters [num_levels, kernel_size].
        gate_weights: Cross-scale gate weights [num_levels, dim].
        num_levels: Number of decomposition levels (default 4).
        kernel_size: Filter kernel size (default 5).
        enforce_qmf: If True, derive high-pass via QMF constraint.

    Returns:
        Transformed sequence [batch, seq_len, dim].

    Raises:
        RuntimeError: If C++ native ops are not available.

    Example:
        >>> x = tf.random.normal([2, 64, 256])
        >>> lp_filters = tf.Variable(tf.random.normal([4, 5]))
        >>> synth = tf.Variable(tf.random.normal([4, 5]))
        >>> gates = tf.Variable(tf.ones([4, 256]) * 0.5)
        >>> output = lmwt_v2_forward(x, lp_filters, synth, gates)
        >>> print(output.shape)  # (2, 64, 256)
    """
    return _ops_lib.LMWTv2Forward(
        x=x,
        low_pass_filters=low_pass_filters,
        synth_filters=synth_filters,
        gate_weights=gate_weights,
        num_levels=num_levels,
        kernel_size=kernel_size,
        enforce_qmf=enforce_qmf,
    )


# =============================================================================
# KERAS LAYER WRAPPER
# =============================================================================


class LearnableLMWTLayer(tf.keras.layers.Layer):
    """Phase 88: Learnable Multi-Scale Wavelet Transformer Layer.

    End-to-end learnable wavelet transform with QMF constraint and
    cross-scale linear attention.

    Attributes:
        num_levels: Number of wavelet decomposition levels.
        kernel_size: Size of wavelet filter kernels.
        enforce_qmf: Whether to enforce QMF constraint.
    """

    def __init__(
        self,
        embedding_dim: int,
        num_levels: int = 4,
        kernel_size: int = 5,
        enforce_qmf: bool = True,
        **kwargs,
    ):
        """Initialize LearnableLMWTLayer.

        Args:
            embedding_dim: Dimension of input embeddings.
            num_levels: Number of decomposition levels (1-5).
            kernel_size: Filter kernel size (default 5).
            enforce_qmf: Enforce QMF constraint for perfect reconstruction.
        """
        super().__init__(**kwargs)
        self.embedding_dim = embedding_dim
        self.num_levels = min(max(num_levels, 1), 5)
        self.kernel_size = kernel_size
        self.enforce_qmf = enforce_qmf

    def build(self, input_shape):
        """Build learnable filter weights."""
        # Initialize with uniform random values (Haar-like init is done via constraint)
        self.low_pass_filters = self.add_weight(
            name="low_pass_filters",
            shape=(self.num_levels, self.kernel_size),
            initializer="glorot_uniform",
            trainable=True,
        )

        self.synth_filters = self.add_weight(
            name="synth_filters",
            shape=(self.num_levels, self.kernel_size),
            initializer="glorot_uniform",
            trainable=True,
        )

        self.gate_weights = self.add_weight(
            name="gate_weights",
            shape=(self.num_levels, self.embedding_dim),
            initializer="glorot_uniform",
            trainable=True,
        )

        super().build(input_shape)

    def call(self, inputs, training=None):
        """Forward pass through learnable LMWT."""
        return lmwt_v2_forward(
            x=inputs,
            low_pass_filters=self.low_pass_filters,
            synth_filters=self.synth_filters,
            gate_weights=self.gate_weights,
            num_levels=self.num_levels,
            kernel_size=self.kernel_size,
            enforce_qmf=self.enforce_qmf,
        )

    def get_config(self):
        """Get layer configuration."""
        config = super().get_config()
        config.update(
            {
                "embedding_dim": self.embedding_dim,
                "num_levels": self.num_levels,
                "kernel_size": self.kernel_size,
                "enforce_qmf": self.enforce_qmf,
            }
        )
        return config
